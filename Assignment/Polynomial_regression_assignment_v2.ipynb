{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Polynomial regression assignment (with solutions).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6ZVJmk4Zrp-"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "import time\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvWazlnqZrp_"
      },
      "source": [
        "#imports for interactive plots\n",
        "\n",
        "from __future__ import print_function\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRX1TOQdZrp_"
      },
      "source": [
        "#imports for sklearn\n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.kernel_ridge import KernelRidge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8e7mbUTZrp_"
      },
      "source": [
        "In this problem we will learn how to use polynomial features to learn nonlinear classification\n",
        "boundaries.\n",
        "Previously, we found that linear regression can\n",
        "be quite effective for classification. We applied it in the setting where the training data points\n",
        "were approximately linearly separable. The term “linearly separable” in classification means there\n",
        "exists a hyperplane such that most of the training data points in the first class are on one side of the\n",
        "hyperplane and most training data points in the second class are on the other side of the hyperplane.\n",
        "\n",
        "However, often times in practice classification datasets are not linearly separable in their native\n",
        "representation. In such cases we can often find a representation that is linearly separable by augmenting the original data with appropriate features. This “lifting” embeds the data points into a higher\n",
        "dimensional space where they are more likely to be linearly separable.\n",
        "In this problem we consider several simple datasets of points (x1\n",
        ", x2) ∈ R^2\n",
        ", each associated with a binary\n",
        "label bi which is −1 or +1. We then apply what we learned on a multi-class classification problem with 11 raw features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA_g2ty_AjAu"
      },
      "source": [
        "# Section 0: Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXo8vNGBIGTC"
      },
      "source": [
        "**Part 1, 2, 3**: In this part, we provide some helper functions for regression problems. Complete the missing codes in assemble_feature(),poly_regression(), classify() and accuracy_percentage() based on the description of the functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYuNZ0RhZrp_"
      },
      "source": [
        "# helper functions\n",
        "def draw_boundary(w,tol=1e-4,reg_type='linear',degree=2,n_grid =1000): \n",
        "    \"\"\"this function draws the regression decision boundary\"\"\"\n",
        "    x1_min = -2\n",
        "    x1_max = 2\n",
        "    x2_min =-2\n",
        "    x2_max =2\n",
        " \n",
        "    grid = np.zeros((n_grid*n_grid,2))\n",
        "    temp1 = np.linspace(x1_min,x1_max,n_grid)\n",
        "    temp2 = np.linspace(x2_min,x2_max,n_grid)\n",
        "    \n",
        "    for i in range(n_grid):\n",
        "        grid[i*n_grid:(i+1)*n_grid,0] =temp1\n",
        "        grid[i*n_grid:(i+1)*n_grid,1] = np.ones(n_grid)*temp2[i]\n",
        "        \n",
        "    if reg_type=='linear':    \n",
        "        indicators =np.abs(grid.dot(w))<tol \n",
        "    elif reg_type=='poly':\n",
        "        phi_grid = assemble_feature(grid,degree)\n",
        "        indicators =np.abs(phi_grid.dot(w))<tol\n",
        "\n",
        "    plt.scatter(grid[indicators, 0], grid[indicators, 1])\n",
        "    pass\n",
        "\n",
        "\n",
        "def draw_boundary_kernel(w,X_train,tol=1e-4,reg_type='linear',degree=2,n_grid =200): \n",
        "    \"\"\" this function draws the kernel regression decision boundary\"\"\"\n",
        "    x1_min = -2\n",
        "    x1_max = 2\n",
        "    x2_min =-2\n",
        "    x2_max =2\n",
        "     \n",
        "    grid = np.zeros((n_grid*n_grid,2))\n",
        "    temp1 = np.linspace(x1_min,x1_max,n_grid)\n",
        "    temp2 = np.linspace(x2_min,x2_max,n_grid)\n",
        "    \n",
        "    for i in range(n_grid):\n",
        "        grid[i*n_grid:(i+1)*n_grid,0] =temp1\n",
        "        grid[i*n_grid:(i+1)*n_grid,1] = np.ones(n_grid)*temp2[i]\n",
        "        \n",
        "    if reg_type=='linear':    \n",
        "        indicators =np.abs(grid.dot(w))<tol \n",
        "    elif reg_type=='poly':\n",
        "        phi_grid = assemble_feature(grid,degree)\n",
        "        indicators =np.abs(phi_grid.dot(w))<tol\n",
        "    elif reg_type=='poly_kernel':\n",
        "        pred = poly_kernel(grid, X_train.T, degree) @ w\n",
        "        indicators = np.abs(pred)<tol\n",
        "    plt.scatter(grid[indicators, 0], grid[indicators, 1])\n",
        "    pass\n",
        "\n",
        "\n",
        "def poly_regression(X_train, y_train, X_test, lambda_=0):\n",
        "    \"\"\" this function solve regression problem with X_train and y_train, and then makes a prediction with X_test \"\"\"\n",
        "    \"\"\" returns both the predicted values and the weight used \"\"\"\n",
        "    \"\"\" we need to use ridge regression later in the this problem.\"\"\"\n",
        "    \"\"\" for this, you will need to add a factor of \"lambda_ * identity to X.T @ X before inverting\" \"\"\"\n",
        "    #TO DO 0 start\n",
        "\n",
        "    #TO DO 0 end\n",
        "    return y_predict, w\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def classify(ypred):\n",
        "    '''This function classifies the predicted vector ypred into two classes {-1, +1}'''\n",
        "    #TO DO 1 start\n",
        "\n",
        "    #TO DO 1 end\n",
        "    return ypred\n",
        "\n",
        "def accuracy_percentage(y_true, y_predict):\n",
        "    '''This function calculates percentage of correct prediction'''\n",
        "    #TO DO 2 start\n",
        "\n",
        "    #TO DO 2 end\n",
        "\n",
        "def assemble_feature(x, D):\n",
        "    '''This function takes in the raw data matrix and return a data matrix with d degree polynomials\n",
        "       For example, if input is (x1, x2) and D =2, output will be a matrix consisting of columns [1,x1,x2,x1*x2,x1^2,x2^2]\n",
        "       Do not use any sklearn functions to do this. We want you to explicitly construct the monomials'''\n",
        "    #TO DO 3 start\n",
        "\n",
        "    #TO DO 3 end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIw4a7I6Zrp_"
      },
      "source": [
        "# Section 1: Building the algorithm with Circle data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJcGzxykZrp_"
      },
      "source": [
        "**Part 4(i)** Visualize the data: Let's import our Circle data and visualize it. Do you expect linear or polynomial regression to work better? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqevdjDkZrp_"
      },
      "source": [
        "# Generate data and relabel points so classes are {-1, +1}\n",
        "X_cir = np.load(\"X_circle.npy\")\n",
        "y_cir = np.load(\"y_circle.npy\")\n",
        "n = y_cir.shape[0]\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "ratio_train = 0.625\n",
        "ratio_val =0.125\n",
        "ratio_test =0.25\n",
        "n_train = int(ratio_train*n)\n",
        "n_val = int(ratio_val*n)\n",
        "n_test = int(ratio_test*n)\n",
        "X_cir_train = X_cir[0:n_train, 0:2]\n",
        "y_cir_train = y_cir[0:n_train]\n",
        "\n",
        "X_cir_val = X_cir[n_train:n_train+n_val, 0:2]\n",
        "y_cir_val = y_cir[n_train:n_train+n_val]\n",
        "\n",
        "X_cir_test = X_cir[n_train+n_val:, 0:2]\n",
        "y_cir_test = y_cir[n_train+n_val:]\n",
        "\n",
        "# Visualize data\n",
        "def visualize_dataset(X, y):\n",
        "    plt.scatter(X[y < 0.0, 0], X[y < 0.0, 1])\n",
        "    plt.scatter(X[y > 0.0, 0], X[y > 0.0, 1])\n",
        "    plt.show()\n",
        "\n",
        "visualize_dataset(X_cir, y_cir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd-oFKdB6OfP"
      },
      "source": [
        "**Part 4(i)**: [[Your observations]] \n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoQFtR8BZrqC"
      },
      "source": [
        "**Part 4(ii)**: Complete the following part to see what happens when we do regression directly with raw features. You may find the helper functions in Section 0 useful. What do you see? Is this consistent with what you expected?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjQXSSBsZrqC"
      },
      "source": [
        "# Model a linear regression\n",
        "\n",
        "#using linear regression to train the training data set and make prediction on the test set\n",
        "# TO DO 4 start \n",
        "\n",
        "# TO DO 4 end\n",
        "\n",
        "print(\"Test accuracy on the test set is \", prediction_acc)\n",
        "draw_boundary(w1,1e-4)\n",
        "visualize_dataset(X_cir, y_cir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ6E3Rig6EEF"
      },
      "source": [
        "**Part 4(ii)**: [[Your observations]]\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPgU-BfaZrqC"
      },
      "source": [
        "**Part 5**: Now complete the missing codes to lift our data with polynomial features, and see how it helps with learning process. Feel free to play with the degree of the polynomial features but report the test accuracy for a 2-degree polynomial. Is this consistent with what you expected?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCKZZylFZrqC"
      },
      "source": [
        "degree_poly = 2\n",
        "\n",
        "# Model a 2-degree polynomial regression\n",
        "# TO DO 5 start\n",
        "\n",
        "# TO DO 5 end\n",
        "\n",
        "print(\"Test accuracy on the test set is \", prediction_acc)\n",
        "draw_boundary(w_cir,1e-3,'poly',degree_poly)\n",
        "visualize_dataset(X_cir, y_cir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQWJyYTC8QRE"
      },
      "source": [
        "**Part 5**: [[Your observations]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULcs5ZA8ZrqD"
      },
      "source": [
        "**Part 6**: Below is an interactive plot showing the decision boundary with different degree of the polynomial features. Makes observations on how the decision boundary change as you increase the degree. Also, pay attention to the run time as the degree increases. Write your observations and comments below "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZKa1o_rZrqD"
      },
      "source": [
        "def interactive_poly(degree_poly):\n",
        "\n",
        "    Phi_cir = assemble_feature(X_cir_train, degree_poly)\n",
        "    Phitest_cir = assemble_feature(X_cir_test, degree_poly) \n",
        "\n",
        "    y_predict_cir, w_cir = poly_regression(Phi_cir, y_cir_train, Phitest_cir, lambda_=0)\n",
        "    acc_perc = accuracy_percentage(y_cir_test, classify(y_predict_cir))\n",
        "\n",
        "    print(\"Test accuracy on the test set is \", acc_perc)\n",
        "    draw_boundary(w_cir,1e-2,'poly',degree_poly)\n",
        "    visualize_dataset(X_cir, y_cir)\n",
        "    pass\n",
        "\n",
        "interact(interactive_poly,degree_poly=widgets.IntSlider(min=1, max=10, step=1, value=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaN16Hsv8JyE"
      },
      "source": [
        "**Part 6**: [[Your observations]] \n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpilHYPBpOgy"
      },
      "source": [
        "**Part 7**: Try different degrees of polynomials and try to find best degree using the validation process. Complete the following code to generate a plot of the prediction accuracy vs the degree of the polynomial. Comment on the difference between train accuracy and validation accuracy. Also write your observations on the run time plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnt9K5kZZrqE"
      },
      "source": [
        "LAMBDA = 0\n",
        "max_d =16\n",
        "\n",
        "accs_train =[]\n",
        "accs_val =[]\n",
        "run_times=[]\n",
        "for D in range(1, max_d):\n",
        "    start_time = time.process_time()\n",
        "    # Model a polynomial regression using train data and test on validation data\n",
        "    # TO DO 7 start:\n",
        "\n",
        "    # TO DO 7 end:\n",
        "\n",
        "    end_time = time.process_time()\n",
        "    run_times.append(end_time-start_time)\n",
        "    accs_train.append(acc_train)\n",
        "    accs_val.append(acc_val)\n",
        "\n",
        "# Plot train accuracy as a function of polynomial degree\n",
        "plt.plot(range(1,max_d),accs_train)\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Train accuracy (%)')\n",
        "plt.title('Train Accuracy vs the Degree')\n",
        "plt.show();\n",
        "\n",
        "# Plot validation accuracy as a function of polynomial degree\n",
        "plt.plot(range(1,max_d),accs_val)\n",
        "plt.title('Validation Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Validation accuracy (%)')\n",
        "plt.show();\n",
        "\n",
        "# Plot run time as a function of polynomial degree\n",
        "plt.plot(range(1,max_d),run_times)\n",
        "plt.title('Run time vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Run time (s)')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3P7_Yty_nLJ"
      },
      "source": [
        "**Part 7**: What is the best degree for this mdoel? Does this makes sense to you intuitively based on the shape of this data set?  Comment on how run time changes with the degree. Why is this happening?\n",
        "\n",
        "[[Your observations]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mpmM97F7PpS"
      },
      "source": [
        "**Part 8**: Complete the following part to train on train and validation set, and evaluate on test set. Report the test accuracy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mm846CgZrqE"
      },
      "source": [
        "# Train model on train and validation set, evaluate on test set\n",
        "\n",
        "D = 3\n",
        "# TO DO 8 start:\n",
        "\n",
        "# TO DO 8 end:\n",
        "\n",
        "print(\"Prediction accuracy on the test set is \", accuracy_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUTg3VoFHX2b"
      },
      "source": [
        "**Part 9**: Now lets compare regression with polynomial features with polynomial kernel regression. Fill in the polynomial kernel equation i.e. create the gram matrix. Run the following codes, and write down your observations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF5d4SzIn5o1"
      },
      "source": [
        "def poly_kernel(X, XT, D):\n",
        "    \"\"\"Create the polynomial order D kernel matrix from X and X^T\"\"\"\n",
        "    #TO DO 9 start:\n",
        "\n",
        "    #TO DO 9 end:\n",
        "    return K\n",
        "\n",
        "# Helper function for interactive plot\n",
        "def interactive_poly_kernel(D):\n",
        "  LAMBDA =0.001\n",
        "  K_cir = poly_kernel(X_cir_train, X_cir_train.T, D) + LAMBDA * np.eye(X_cir_train.shape[0])\n",
        "  w_cir_k = np.linalg.solve(K_cir, y_cir_train)\n",
        "\n",
        "  ypred_train_cir_k = poly_kernel(X_cir_train, X_cir_train.T, D) @ w_cir_k\n",
        "  acc_train = accuracy_percentage(y_cir_train, classify(ypred_train_cir_k))\n",
        "  print(\"Prediction accuracy on the train set is \", acc_train)\n",
        "\n",
        "  ypred_test_cir_k = poly_kernel(X_cir_test, X_cir_train.T, D) @ w_cir_k\n",
        "  acc_test = accuracy_percentage(y_cir_test, classify(ypred_test_cir_k))\n",
        "  print(\"Prediction accuracy on the test set is \", acc_test)\n",
        "\n",
        "\n",
        "  draw_boundary_kernel(w_cir_k,X_cir_train,1e-1,'poly_kernel',D,200)\n",
        "  visualize_dataset(X_cir, y_cir)\n",
        "  pass\n",
        "\n",
        "# Visualize interactive plot for boundary by polynomial degree\n",
        "interact(interactive_poly_kernel,D=widgets.IntSlider(min=1, max=10, step=1, value=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUCLUqPPHlMd"
      },
      "source": [
        "**Part 9**:\n",
        "[[Your observations]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxtBoVSmXpQW"
      },
      "source": [
        "**Part 10**: Run the following codes to plot accuracy and run time vs the degree for kernel regression method. Compare and contrast accuracy and run time between polynomial kernel regression and regression with polynomial features. Write down your observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D60-fFYtXfRP"
      },
      "source": [
        "LAMBDA = 0.001\n",
        "max_d =16\n",
        "\n",
        "accs_train_k =[]\n",
        "accs_val_k =[]\n",
        "run_times_k=[]\n",
        "\n",
        "# Model polynomial regression with degree 1 through max_d for circle data\n",
        "for D in range(1, max_d):\n",
        "    start_time = time.process_time()\n",
        "\n",
        "    K_cir = poly_kernel(X_cir_train, X_cir_train.T, D) + LAMBDA * np.eye(X_cir_train.shape[0])\n",
        "    w_cir_k = np.linalg.solve(K_cir, y_cir_train)\n",
        "\n",
        "    ypred_train_cir_k = poly_kernel(X_cir_train, X_cir_train.T, D) @ w_cir_k\n",
        "    acc_train = accuracy_percentage(y_cir_train, classify(ypred_train_cir_k))\n",
        "\n",
        "    ypred_val_cir_k = poly_kernel(X_cir_val, X_cir_train.T, D) @ w_cir_k\n",
        "    acc_val = accuracy_percentage(y_cir_val, classify(ypred_val_cir_k))\n",
        "\n",
        "\n",
        "    end_time = time.process_time()\n",
        "    run_times_k.append(end_time-start_time)\n",
        "    accs_train_k.append(acc_train)\n",
        "    accs_val_k.append(acc_val)\n",
        "\n",
        "# Plot train accuracy as a function of degree\n",
        "plt.plot(range(1,max_d),accs_train_k, label='Poly kernel regression')\n",
        "plt.plot(range(1,max_d),accs_train,label='Regression with poly features')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Train accuracy (%)')\n",
        "plt.title('Train Accuracy vs the Degree')\n",
        "plt.legend(fontsize=12)\n",
        "plt.show();\n",
        "\n",
        "# Plot validation accuracy as a function of degree\n",
        "plt.plot(range(1,max_d),accs_val_k,label='Poly kernel regression')\n",
        "plt.plot(range(1,max_d),accs_val,label='Regression with poly features')\n",
        "plt.title('Validation Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Validation accuracy (%)')\n",
        "plt.legend(fontsize=12)\n",
        "plt.show();\n",
        "\n",
        "# Plot run time as a function of degree\n",
        "plt.plot(range(1,max_d),run_times_k,label='Poly kernel regression')\n",
        "plt.plot(range(1,max_d),run_times,label='Regression with poly features')\n",
        "plt.title('Run time vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Run time (s)')\n",
        "plt.legend(fontsize=12)\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfdkpfQXbJFF"
      },
      "source": [
        "**Part 10**:\n",
        "[[Your observations]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et8p-f8lB8ZB"
      },
      "source": [
        "# Section 2: Star data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIKHuLHa9rW7"
      },
      "source": [
        "Part 11: Let's try our developed algorithm with more complicated data boundary. Here is a different data set. Run the following block of codes to visualize the data. Intuitvely, what is your guess for best degree in this case. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKUtzS5HFDMY"
      },
      "source": [
        "# Load data\n",
        "X_star = np.load(\"X_star.npy\")\n",
        "y_star = np.load(\"y_star.npy\")\n",
        "\n",
        "# Split into train and test data\n",
        "ratio_train = 0.8\n",
        "ratio_test =0.2\n",
        "\n",
        "n_train = int(ratio_train*len(y_star))\n",
        "\n",
        "n_test = int(ratio_test*len(y_star))\n",
        "X_star_train = X_star[0:n_train, 0:2]\n",
        "y_star_train = y_star[0:n_train]\n",
        "\n",
        "X_star_test = X_star[n_train:, 0:2]\n",
        "y_star_test = y_star[n_train:]\n",
        "\n",
        "# Visualize data\n",
        "visualize_dataset(X_star, y_star)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKEeY7ut0X_2"
      },
      "source": [
        "**Part 11**: [[Your observations]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjANQ3DVQkwK"
      },
      "source": [
        "**Part 12**: Play with the interactive plot below and also study the accuracy plots. What is the best degree in this case? Why is it different from the best degree for previous data set? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjwF8GUpoOFg"
      },
      "source": [
        "LAMBDA = 0.001\n",
        "max_d =16\n",
        "\n",
        "accs_train_star =[]\n",
        "accs_val_star =[]\n",
        "\n",
        "# Model star data boundary using polynomial regression with degree 1 through max_d\n",
        "for D in range(1, max_d):\n",
        "    # Use poly kernel to run a polynomial regression\n",
        "    K_star = poly_kernel(X_star_train, X_star_train.T, D) + LAMBDA * np.eye(X_star_train.shape[0])\n",
        "    w_star = np.linalg.solve(K_star, y_star_train)\n",
        "\n",
        "    ypred_train_k = poly_kernel(X_star_train, X_star_train.T, D) @ w_star\n",
        "    acc_train = accuracy_percentage(y_star_train, classify(ypred_train_k))\n",
        "\n",
        "    ypred_test_k = poly_kernel(X_star_test, X_star_train.T, D) @ w_star\n",
        "    acc_test = accuracy_percentage(y_star_test, classify(ypred_test_k))\n",
        "\n",
        "    accs_train_star.append(acc_train)\n",
        "    accs_val_star.append(acc_test)\n",
        "\n",
        "# Plot accuracies for Circle and Star data\n",
        "plt.plot(range(1,max_d),accs_train_k, label='Circular Data')\n",
        "plt.plot(range(1,max_d),accs_train_star,label='Star Data')\n",
        "plt.title('Train Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Train accuracy (%)')\n",
        "plt.legend(fontsize=12)\n",
        "plt.show();\n",
        "\n",
        "plt.plot(range(1,max_d),accs_val_k,label='Circular Data')\n",
        "plt.plot(range(1,max_d),accs_val_star,label='Star Data')\n",
        "plt.title('Validation Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Validation accuracy (%)')\n",
        "plt.legend(fontsize=12)\n",
        "plt.show();\n",
        "\n",
        "# Visualize interactive plot for how the boundary changes with respect to polynomial degree\n",
        "def interactive_star(D):\n",
        "  K_star = poly_kernel(X_star_train, X_star_train.T, D) + LAMBDA * np.eye(X_star_train.shape[0])\n",
        "  w_star = np.linalg.solve(K_star, y_star_train)\n",
        "\n",
        "  ypred_star_test = poly_kernel(X_star_test, X_star_train.T, D) @ w_star\n",
        "  acc_star = accuracy_percentage(y_star_test, classify(ypred_star_test))\n",
        "  print(\"prediction accuracy on the train set is \", acc_star)\n",
        "\n",
        "\n",
        "  draw_boundary_kernel(w_star,X_star_train,1e-1,'poly_kernel',D,200)\n",
        "  visualize_dataset(X_star, y_star)\n",
        "  pass\n",
        "\n",
        "interact(interactive_star,D=widgets.IntSlider(min=1, max=16, step=1, value=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mtAczLYQmXK"
      },
      "source": [
        "**Part 12**: [[Your observations]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11UHjaAeH-6N"
      },
      "source": [
        "# Section 3: Multiclass problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KkAXnZ62T63"
      },
      "source": [
        "Part 13: Instead of two classes, we will have data points in four classes in this new dataset. The four class labels are -3,-1,1, and 3. That is, the class label yi for data point xi will take one of the above values depending on its class. Run the code below to visualize the data. Intuitively, what do you think the best degree will be?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xni8ssaf3XDh"
      },
      "source": [
        "# Load four class data\n",
        "X_four = np.load(\"X_four.npy\")\n",
        "y_four = np.load(\"y_four.npy\")\n",
        "\n",
        "# Split data into train and test\n",
        "ratio_train = 0.8\n",
        "ratio_test =0.2\n",
        "\n",
        "n_train = int(ratio_train*len(y_four))\n",
        "\n",
        "n_test = int(ratio_test*len(y_four))\n",
        "X_four_train = X_four[0:n_train, 0:2]\n",
        "y_four_train = y_four[0:n_train]\n",
        "\n",
        "X_four_test = X_four[n_train:, 0:2]\n",
        "y_four_test = y_four[n_train:]\n",
        "\n",
        "# Visualize data\n",
        "plt.scatter(X_four[y_four==-3, 0], X_four[y_four==-3, 1])\n",
        "plt.scatter(X_four[y_four==-1, 0], X_four[y_four==-1, 1])\n",
        "plt.scatter(X_four[y_four==1, 0], X_four[y_four==1, 1])\n",
        "plt.scatter(X_four[y_four==3, 0], X_four[y_four==3, 1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0c1w1pWQd0j"
      },
      "source": [
        "**Part 13**: [[Your observations]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLMyRnm77y5y"
      },
      "source": [
        "**Part 14**: Complete the classification function classify_four_classes(ypred) and run the following codes. Verify your guess on the best degree. Comment on what you find.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5C-f0j_nGMc"
      },
      "source": [
        "def classify_four_classes(ypred):\n",
        "# Classify ypred into labels {-3, -1, +1, +3} by creating reasonable boundaries\n",
        "    # TO DO 14 start: \n",
        "\n",
        "    # TO DO 14 end:\n",
        "    \n",
        "    return ypred\n",
        "\n",
        "# Visualize the true labels\n",
        "plt.scatter(X_four[y_four==-3, 0], X_four[y_four==-3, 1])\n",
        "plt.scatter(X_four[y_four==-1, 0], X_four[y_four==-1, 1])\n",
        "plt.scatter(X_four[y_four==1, 0], X_four[y_four==1, 1])\n",
        "plt.scatter(X_four[y_four==3, 0], X_four[y_four==3, 1])\n",
        "plt.title('Ground-truth Classification')\n",
        "plt.show()\n",
        "\n",
        "# Visualize how the classes change based on our polynomial regression model\n",
        "LAMBDA = 0.001\n",
        "def interactive_circles(D):\n",
        "  K_four = poly_kernel(X_four_train, X_four_train.T, D) + LAMBDA * np.eye(X_four_train.shape[0])\n",
        "  w_four = np.linalg.solve(K_four, y_four_train)\n",
        "\n",
        "  ypred_four_test = classify_four_classes(poly_kernel(X_four_test, X_four_train.T, D) @ w_four)\n",
        "  acc_four = accuracy_percentage(y_four_test, ypred_four_test)\n",
        "  \n",
        "  ypred_four = classify_four_classes(poly_kernel(X_four, X_four_train.T, D) @ w_four)\n",
        "\n",
        "  plt.scatter(X_four[ypred_four==-3, 0], X_four[ypred_four==-3, 1])\n",
        "  plt.scatter(X_four[ypred_four==-1, 0], X_four[ypred_four==-1, 1])\n",
        "  plt.scatter(X_four[ypred_four==1, 0], X_four[ypred_four==1, 1])\n",
        "  plt.scatter(X_four[ypred_four==3, 0], X_four[ypred_four==3, 1])\n",
        "  plt.title('Learned Classification')\n",
        "  plt.show()\n",
        "  print(\"Prediction accuracy on the test set is \", acc_four)\n",
        "  pass\n",
        "\n",
        "interact(interactive_circles,D=widgets.IntSlider(min=1, max=16, step=1, value=2))\n",
        "\n",
        "max_d =18\n",
        "accs_train_four =[]\n",
        "accs_test_four =[]\n",
        "\n",
        "# Model a polynomial regression for degrees 1 through max_d\n",
        "for D in range(1, max_d):\n",
        "    K_four = poly_kernel(X_four_train, X_four_train.T, D) + LAMBDA * np.eye(X_four_train.shape[0])\n",
        "    w_four = np.linalg.solve(K_four, y_four_train)\n",
        "\n",
        "    ypred_train_k = classify_four_classes(poly_kernel(X_four_train, X_four_train.T, D) @ w_four)\n",
        "    acc_train = accuracy_percentage(y_four_train, ypred_train_k)\n",
        "\n",
        "    ypred_test_k = classify_four_classes(poly_kernel(X_four_test, X_four_train.T, D) @ w_four)\n",
        "    acc_test = accuracy_percentage(y_four_test, ypred_test_k)\n",
        "\n",
        "    acc_train = accuracy_percentage(y_four_train, ypred_train_k)\n",
        "    acc_test = accuracy_percentage(y_four_test, ypred_test_k)\n",
        "\n",
        "    accs_train_four.append(acc_train)\n",
        "    accs_test_four.append(acc_test)\n",
        "\n",
        "# Visualize accuracy rates as a function of polynomial degree\n",
        "plt.plot(range(1,max_d),accs_train_four,label='Four Classes')\n",
        "plt.title('Train Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Train accuracy (%)')\n",
        "plt.show();\n",
        "\n",
        "plt.plot(range(1,max_d),accs_test_four,label='Four Classes')\n",
        "plt.title('Validation Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Validation accuracy (%)')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6e0bhOk8Khh"
      },
      "source": [
        "**Part 14**: [[Your observations]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0l8RnZdZrqE"
      },
      "source": [
        "#Section 4: Training with wine data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U24To8vP4UY"
      },
      "source": [
        " The wine quality dataset archives a numerical description to 1600 different kinds of wine. There are 12 numerical values to describe the property of each wine in the dataset. The first 11 values can be seen as the input features. While the twelfth value is a integer output label varies from from 1 to 10, which evaluates the quality of each wine. Our goal is to use the 11-D input feature vector and the 1-D output label to train a machine learning model, using polynomial regression techniques. \n",
        "\n",
        "In this question, you will start with pre-processing a real-world dataset to have a basic understanding to the dataset and the task. Then we will walk you through some tricky problems in machine learning with the problem setting and the starter code, to gain you some intuition on how to deal with the defects of a dataset. At the end we want you to compare the accuracy and the efficiency of different machine learning implementation by observing the behaviors of them. Working on the question will help you roughly see how real-world machine learning would be like. \n",
        "\n",
        "This dataset is available from the UCI machine learning repository (see https://archive.ics.uci.edu/ml/datasets/wine+quality).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnTVNc51Rz4T"
      },
      "source": [
        "First let's load the data. Before applying any learning algorithm to the wine dataset, we will first preprocess the dataset. In the real-world, pre-processing the data is a very important step since real-life data can be quite imperfect. Since the raw data matrix is not normalized and contains large values that can cause overflow. We will first rescale each column of the data matrix to the same scale and then perform regression on the new data matrix. Complete the function rescale_matrix() and run the following code to load the data. Play with interactive plot to visualize data with two selected features. Based on your observations on the interactive plot, do you think this data set is linearly separable? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0xR7YG5oGH3"
      },
      "source": [
        "X_wine = np.load(\"X_wine.npy\")\n",
        "X_wine_train = np.load(\"X_wine_train.npy\")\n",
        "X_wine_test = np.load(\"X_wine_test.npy\")\n",
        "X_wine_val = np.load(\"X_wine_val.npy\")\n",
        "\n",
        "y_wine = np.load(\"y_wine.npy\")\n",
        "y_wine_train = np.load(\"y_wine_train.npy\")\n",
        "y_wine_test = np.load(\"y_wine_test.npy\")\n",
        "y_wine_val = np.load(\"y_wine_val.npy\")\n",
        "\n",
        "# rescale the range of each feature to be from minValue and maxValue \n",
        "def rescale_matrix(X,maxValue,minValue):\n",
        "  n1,n2 = X.shape\n",
        "  Xp = np.zeros(X.shape)\n",
        "  # in the following section, you will rescale the data matrix and save it as Xp\n",
        "  # In Xp, each feature has maximal feature value equals to maxValue, and minimal value equals to minValue\n",
        "  #TO DO 15 start:\n",
        "  \n",
        "  #TO DO 15 end:\n",
        "  return Xp\n",
        "\n",
        "maxValue = 2\n",
        "minValue =-2\n",
        "X_wine = rescale_matrix(X_wine,minValue,maxValue)\n",
        "X_wine_train = rescale_matrix(X_wine_train,minValue,maxValue)\n",
        "X_wine_val = rescale_matrix(X_wine_val,minValue,maxValue)\n",
        "X_wine_test = rescale_matrix(X_wine_test,minValue,maxValue)\n",
        "\n",
        "def visualize_wine_2D(X,y,feature1,feature2):\n",
        "  # feature1 and feature2 are indices from 0 to 10\n",
        "  labels =['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','total sulfur dioxide','density','pH','sulphates','alcohol']\n",
        "\n",
        "  n1,n2 = X.shape\n",
        "\n",
        "  x1 = X[:,feature1]\n",
        "  x2 = X[:,feature2]\n",
        "\n",
        "  for i in range(10):\n",
        "    plt.scatter(x1[y==i], x2[y==i])\n",
        "  plt.title('Data with '+labels[feature1]+' and '+labels[feature2])\n",
        "  plt.show()\n",
        "  pass\n",
        "\n",
        "def interactive_visualize_wine_2D(feature1,feature2):\n",
        "  visualize_wine_2D(X_wine_train,y_wine_train,feature1,feature2)\n",
        "\n",
        "interact(interactive_visualize_wine_2D, feature1=widgets.IntSlider(min=0, max=10, step=1, value=0), feature2=widgets.IntSlider(min=0, max=10, step=1, value=1)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adh9_HatQAOP"
      },
      "source": [
        "**Part 16**: [[Your observations]]\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V7CgUmN3LQT"
      },
      "source": [
        "Now let's try learn the data using regression with polynomial features. The output of regression is a continuous variable. Complete the function classify_wine() below to convert the continuous variable into the wine quality score (ie. the class label for each data point). Also, complete the missing codes in the function visualize_wine_regression_results() to do regression with polynomial on this data set. Run the code and make your observations. The interactive plot shows the prediction labels vs the ground-truth label for selected degree of polynomial. Play with this plot, and report the best degree.Comment on effectiveness on training this data set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZenMib4ye7k"
      },
      "source": [
        "\n",
        "def classify_wine(y):\n",
        "  # This function take the learned y value, and output the classified ypred into 10 classes\n",
        "  #TODO 17 start\n",
        "\n",
        "  #TODO 17 end\n",
        "  return ypred.astype(int)\n",
        "\n",
        "def visualize_wine_regression_results(X_train,y_train,D,X_test,y_test):\n",
        "  #TODO 18 start\n",
        "\n",
        "  #TODO 18 end\n",
        "\n",
        "  for i in range(10):\n",
        "    plt.scatter(y_test[y_test==i],ypoly[y_test==i])\n",
        "  plt.title('Prediction on test data with '+ str(D) + ' degree polynomial features')\n",
        "  plt.ylabel('Predicted quality values')\n",
        "  plt.xlabel('Actual wine quality')\n",
        "  plt.xlim(0,10)\n",
        "  plt.ylim(0,10)\n",
        "  plt.show() \n",
        "\n",
        "  acc_wine_test = accuracy_percentage(y_test,classify_wine(ypoly))\n",
        "  print('Prediction accuracy on test data: ',acc_wine_test) \n",
        "  pass\n",
        "\n",
        "def interactive_visualize_wine_regression_results(D):\n",
        "  visualize_wine_regression_results(X_wine_train,y_wine_train,D,X_wine_test,y_wine_test)\n",
        " \n",
        "interact(interactive_visualize_wine_regression_results, D = widgets.IntSlider(min=1, max=12, step=1, value=1)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmCb1ujzR0mY"
      },
      "source": [
        "**Part 19**: [[Your obervations on the prediction results]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCONz_0-37En"
      },
      "source": [
        "Plot histogram of the data labels. Write down your observations. Based on your observations, what could be one of the reasons that causes the low prediction accuracy in the previous cell?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVmyPPFt37qH"
      },
      "source": [
        "# plot a historgram of the original data\n",
        "# TO DO 20\n",
        "\n",
        "# TO DO 20\n",
        "\n",
        "plt.xlabel('Wine quality scores')\n",
        "plt.ylabel('Count')\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoQAS1jmUr2c"
      },
      "source": [
        "**Part 20**: [[Your answer]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMMbyCr73u69"
      },
      "source": [
        "Now let's complete the missing code below to rebin the data into two classes (with class labels -1 and 1) so that two classes will have a similar number of data points. The histogram above might give you a hint on how to split the data. Note that in real life, if you are asked to predict wine quality score, this approach is not going to help. You will likely have to use a more complex modeling algorithm. But for this exercise, we will just simplify the problem to be applicable to polynomial regression. The following two parts are aimed to show that a signficant class imbalance can produce poor prediction results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyYZgx9O3t5i"
      },
      "source": [
        "# rebin ten classes into two classes\n",
        "yr_wine =np.zeros(len(y_wine))\n",
        "yr_wine_train=np.zeros(len(y_wine_train))\n",
        "yr_wine_test=np.zeros(len(y_wine_test))\n",
        "yr_wine_val=np.zeros(len(y_wine_val))\n",
        "\n",
        "#TO DO 21 start\n",
        "#  in this part, define yr_wine, yr_wine_train, yr_wine_test and yr_wine_val so that they take on labels {-1, +1}\n",
        "\n",
        "#TO DO 21 end\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cuuEKRBEO3a"
      },
      "source": [
        "**Part 22**: First let's try to learn the rebined wine data using regression with polynomial features.  In this problem, we give you the value for LAMBDA. Make sure to use plug this value in for lambda_ in poly_regression() instead of using the default value when filling the missing code. Compare the performance with before and comment your observations below. Change different degree of polynomials under 10, and report the max degree you can run on this data set. Why do you think training this data set is significantly slower?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOu2yCyfmYB5"
      },
      "source": [
        "Note: in the starter code, we provided the LAMBDA (regularization hyperparameter) value used for ridge regression. You will learn about ridge regression in detail later this week, but for now, all you need to know that using ridge regression helps deal with noise via a concept called \"regularization.\" For real-life data that is always subject to noise, proper regularization is critical to achieving an ideal performance. This hybrid model can remind you that in real-world machine learning practice, you can try any appropriate tool for a task to pursue the best performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NopH-L_4EH1j"
      },
      "source": [
        "# regression with poly features\n",
        "\n",
        "LAMBDA=5000\n",
        "max_d =5\n",
        "\n",
        "accs_train_wine =[]\n",
        "accs_val_wine =[]\n",
        "run_times_wine=[]\n",
        "for D in range(1, max_d):\n",
        "    start_time = time.process_time()\n",
        "\n",
        "    # complete the following part to train on X_wine_train and report predicton accuracy on X_wine_val\n",
        "    # make sure to use ridge regression\n",
        "    # TO DO 22 start:\n",
        "\n",
        "    # TO DO 22 end:\n",
        "\n",
        "    end_time = time.process_time()\n",
        "    run_times_wine.append(end_time-start_time)\n",
        "    accs_train_wine.append(acc_train)\n",
        "    accs_val_wine.append(acc_val)\n",
        "\n",
        "# Plot train accuracy as a function of polynomial degree\n",
        "plt.plot(range(1,max_d),accs_train_wine, label='Your regression with poly features')\n",
        "plt.title('Train Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Train accuracy (%)')\n",
        "plt.show();\n",
        "\n",
        "# Plot validation accuracy as a function of polynomial degree\n",
        "plt.plot(range(1,max_d),accs_val_wine, label='Your regression with poly features')\n",
        "plt.title('Validation Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Validation accuracy (%)')\n",
        "plt.show();\n",
        "\n",
        "# Plot run time as a function of polynomial degree\n",
        "plt.plot(range(1,max_d),run_times_wine, label='Your regression with poly features')\n",
        "plt.title('Run time vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Run time (s)')\n",
        "plt.show();\n",
        "\n",
        "\n",
        "\n",
        "def interactive_wine_acc(D):\n",
        "  Xd_wine_train = assemble_feature(X_wine_train, D)\n",
        "  Xd_wine_test = assemble_feature(X_wine_test, D)\n",
        "  ypred_test,w =poly_regression(Xd_wine_train, yr_wine_train, Xd_wine_test,LAMBDA)\n",
        " \n",
        "  acc_test = accuracy_percentage(yr_wine_test, classify(ypred_test))\n",
        "\n",
        "  print(\"Prediction accuracy on the test set is \", acc_test)\n",
        "\n",
        "\n",
        "  pass\n",
        "\n",
        "interact(interactive_wine_acc,D=widgets.IntSlider(min=1, max=15, step=1, value=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCII4Xit9IVA"
      },
      "source": [
        "**Part 22**: [[Your observations]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB_L3mv4cti-"
      },
      "source": [
        "**Part 23**: Now try to use poly kernel regression to train the rebined wine data set. Are you able to run the train the data with higher degree polynomials? Why? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQh03hRCZrqE"
      },
      "source": [
        "# kernel regression\n",
        "\n",
        "LAMBDA=6000\n",
        "\n",
        "max_d =15\n",
        "\n",
        "accs_train_wine_k =[]\n",
        "accs_val_wine_k =[]\n",
        "run_times_wine_k =[]\n",
        "\n",
        "for D in range(1, max_d):\n",
        "    start_time = time.process_time()\n",
        "\n",
        "    # make sure to use ridge regression\n",
        "    # TO DO 23 start\n",
        "\n",
        "    #TO DO 23 end\n",
        "\n",
        "    end_time = time.process_time()\n",
        "\n",
        "    accs_train_wine_k.append(acc_train)\n",
        "    accs_val_wine_k.append(acc_val)\n",
        "    run_times_wine_k.append(end_time-start_time)\n",
        "\n",
        "# Plot accuracies for Circle and Star data\n",
        "plt.plot(range(1,max_d),accs_train_wine_k, label='Your poly kernel')\n",
        "plt.title('Train Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Train accuracy (%)')\n",
        "plt.legend(fontsize=12)\n",
        "plt.show();\n",
        "\n",
        "plt.plot(range(1,max_d),accs_val_wine_k,label='Your poly kernel')\n",
        "plt.title('Validation Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Validation accuracy (%)')\n",
        "plt.legend(fontsize=12)\n",
        "plt.show();\n",
        "\n",
        "plt.plot(range(1,max_d),run_times_wine_k,label='Your poly kernel')\n",
        "plt.title('Run time vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Run time (s)')\n",
        "plt.legend(fontsize=12)\n",
        "plt.show();\n",
        "\n",
        "\n",
        "def interactive_wine_acc_k(D):\n",
        "  K_wine = poly_kernel(X_wine_train, X_wine_train.T, D) + LAMBDA * np.eye(X_wine_train.shape[0])\n",
        "  w_wine = np.linalg.solve(K_wine, yr_wine_train)\n",
        "\n",
        "  ypred_wine_test = classify(poly_kernel(X_wine_test, X_wine_train.T, D) @ w_wine)\n",
        "  acc_star = accuracy_percentage(yr_wine_test, ypred_wine_test)\n",
        "  print(\"Prediction accuracy on the test set is \", acc_star)\n",
        "\n",
        "  pass\n",
        "\n",
        "interact(interactive_wine_acc_k,D=widgets.IntSlider(min=1, max=15, step=1, value=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFa3CerfdEez"
      },
      "source": [
        "**Part 23**: [[Your observations]]\n",
        "\n",
        "Your answer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeqZA243L7Ju"
      },
      "source": [
        "Scikit-learn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. In general, using a machine learning library will enormously speed up your coding process. You should be able to implement a more complicated machine learning model, i.e. combining more than one machine learning algorithm in a model by simply calling library functions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgNC1674YPoG"
      },
      "source": [
        "The following two parts are aimed to get you familiar with sklearn library. You will read the documentation of sklearn functions to try to implement regression with polynomial features and polynomial kernel regression with sklearn function. You may find the following sections of sklearn library useful: sklearn.preprocessing, sklearn.linear_model.LinearRegression and sklearn.kernel_ridge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8dbqVwoNb1U"
      },
      "source": [
        "Note: As we did in the manual implementation for polynomial regression earlier, we will need to include some regularization via ridge regression when using sklearn to implement polynomial regression. In this case, be sure to use Ridge() which we imported for you and the value for LAMBDA which we also provided for you. Again, you will learn about ridge in more detail later this week so don't worry about the details for now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laMkqXAUdJSK"
      },
      "source": [
        "**Part 24**: First try regression with polynomial features. Complete the missing codes. What is the best D in this case? How does it compare with results from your implementation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJu0VfY0L4Z2"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ALPHA =6000\n",
        "\n",
        "\n",
        "# plot accuracy\n",
        "max_d = 10\n",
        "accs_train_wine_sk = []\n",
        "accs_val_wine_sk = []\n",
        "run_times_wine_sk =[]\n",
        "for D in range(1, max_d):\n",
        "    \n",
        "    start_time = time.process_time()\n",
        "\n",
        "    # make sure to use ridge regression\n",
        "    #TO DO 24 start: \n",
        "\n",
        "    #TO DO 24 end:\n",
        "\n",
        "    end_time = time.process_time()\n",
        "    run_times_wine_sk.append(end_time-start_time)\n",
        "    accs_train_wine_sk.append(acc_wine_train)\n",
        "    accs_val_wine_sk.append(acc_wine_val)\n",
        "    \n",
        "\n",
        "    \n",
        "plt.plot(range(1,max_d),accs_train_wine_sk,label='SK regression with poly features')\n",
        "plt.title('Train Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Train Accuracy (%)')\n",
        "plt.show();\n",
        "\n",
        "plt.plot(range(1,max_d),accs_val_wine_sk,label='SK regression with poly features')\n",
        "plt.title('Validation Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.show();\n",
        "\n",
        "\n",
        "plt.plot(range(1,max_d),run_times_wine_sk,label='SK regression with poly features')\n",
        "plt.title('Run time vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Run Time (s)')\n",
        "plt.show();\n",
        "\n",
        "def visualize_wine_sk_regression(X_train,y_train,D,X_test,y_test):\n",
        "\n",
        "  poly = PolynomialFeatures(D)\n",
        "  phi = poly.fit_transform(X_train)\n",
        "  phi_test =poly.fit_transform(X_test)\n",
        "  reg = Ridge(alpha=ALPHA)\n",
        "  reg.fit(phi, y_train)\n",
        "  ypoly = reg.predict(phi_test)\n",
        "\n",
        "\n",
        "  acc_wine_test = accuracy_percentage(y_test,classify(ypoly))\n",
        "  print('Prediction accuracy on test data: ',acc_wine_test) \n",
        "  pass\n",
        "\n",
        "def interactive_visualize_wine_sk_regression(D):\n",
        "  visualize_wine_sk_regression(X_wine_train,yr_wine_train,D,X_wine_test,yr_wine_test)\n",
        "\n",
        "interact(interactive_visualize_wine_sk_regression,D=widgets.IntSlider(min=2, max=15, step=1, value=2))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VE_PRgqn6Fs"
      },
      "source": [
        "**Part 24**: [[Your answer]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hg1KacWdHFE"
      },
      "source": [
        "**Part 25**: \n",
        "Repeat with sklearn.kernel_ridge to do poly kernel regression with sklearn functions. What is the best D in this case? How does it compare with results from your implementation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibYNKMHOdF8h"
      },
      "source": [
        "# plot accuracy\n",
        "ALPHA =6000\n",
        "max_d = 15\n",
        "accs_train_wine_sk_kernel = []\n",
        "accs_val_wine_sk_kernel = []\n",
        "run_times_wine_sk_kernel = []\n",
        "\n",
        "for D in range(1, max_d):\n",
        "    \n",
        "    start_time = time.process_time()\n",
        "\n",
        "    # make sure to use ridge regression\n",
        "    #TO DO 25 start:\n",
        "\n",
        "    #TO DO  25 end: \n",
        "\n",
        "\n",
        "    end_time = time.process_time()\n",
        "\n",
        "    run_times_wine_sk_kernel.append(end_time-start_time)\n",
        "    accs_train_wine_sk_kernel.append(acc_wine_train)\n",
        "    accs_val_wine_sk_kernel.append(acc_wine_val)\n",
        "    \n",
        "\n",
        "\n",
        "plt.plot(range(1,max_d),accs_train_wine_sk_kernel,label='SK kernel')\n",
        "plt.title('Train Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Train Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.show();\n",
        "\n",
        "\n",
        "plt.plot(range(1,max_d),accs_val_wine_sk_kernel,label='Sk kernel')\n",
        "plt.title('Validation Accuracy vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.show();\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(range(1,max_d),run_times_wine_sk_kernel,label='SK kernel')\n",
        "plt.title('Run Time vs the Degree')\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('Run Time (s)')\n",
        "plt.legend()\n",
        "plt.show();\n",
        "\n",
        "\n",
        "\n",
        "def visualize_wine_sk_kernel(X_train,y_train,D,ALPHA,X_test,y_test):\n",
        " \n",
        "\n",
        "  clf = sk.kernel_ridge.KernelRidge(alpha=ALPHA,kernel='polynomial',degree=D)\n",
        "  clf.fit(X_train, y_train)\n",
        "  ypoly =clf.predict(X_test)\n",
        "\n",
        "\n",
        "  acc_wine_test = accuracy_percentage(y_test,classify(ypoly))\n",
        "  print('Prediction accuracy on test data: ',acc_wine_test) \n",
        "  pass\n",
        "\n",
        "def interactive_visualize_wine_sk_kernel(D):\n",
        "  visualize_wine_sk_kernel(X_wine_train,yr_wine_train,D,ALPHA,X_wine_test,yr_wine_test)\n",
        "\n",
        "interact(interactive_visualize_wine_sk_kernel,D=widgets.IntSlider(min=1, max=15, step=1, value=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMpb5sMeoA13"
      },
      "source": [
        "**Part 25**: [[Your answer]]\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I70boWmZfmli"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}